@misc{chen2020interpretable,
    abstract = {Unlike popular modularized framework, end-to-end autonomous driving seeks to solve the perception, decision and control problems in an integrated way, which can be more adapting to new scenarios and easier to generalize at scale. However, existing end-to-end approaches are often lack of interpretability, and can only deal with simple driving tasks like lane keeping. In this paper, we propose an interpretable deep reinforcement learning method for end-to-end autonomous driving, which is able to handle complex urban scenarios. A sequential latent environment model is introduced and learned jointly with the reinforcement learning process. With this latent model, a semantic birdeye mask can be generated, which is enforced to connect with a certain intermediate property in today's modularized framework for the purpose of explaining the behaviors of learned policy. The latent space also significantly reduces the sample complexity of reinforcement learning. Comparison tests with a simulated autonomous car in CARLA show that the performance of our method in urban scenarios with crowded surrounding vehicles dominates many baselines including DQN, DDPG, TD3 and SAC. Moreover, through masked outputs, the learned policy is able to provide a better explanation of how the car reasons about the driving environment. The codes and videos of this work are available at our github repo and project website.},
    author = {Jianyu Chen and Shengbo Eben Li and Masayoshi Tomizuka},
    keywords = {Deep reinforcement learn-ing,End-to-end driving policy,Index Terms-Autonomous driving,Interpretability,Probabilistic graphical model},
    month = {1},
    title = {Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning},
    url = {http://arxiv.org/abs/2001.08726},
    year = {2020},
}

@article{peng2021end,
    abstract = {<p> Recent years have seen the rapid development of autonomous driving systems, which are typically designed in a hierarchical architecture or an end-to-end architecture. The hierarchical architecture is always complicated and hard to design, while the end-to-end architecture is more promising due to its simple structure. This paper puts forward an end-to-end autonomous driving method through a deep reinforcement learning algorithm Dueling Double Deep Q-Network, making it possible for the vehicle to learn end-to-end driving by itself. This paper firstly proposes an architecture for the end-to-end lane-keeping task. Unlike the traditional image-only state space, the presented state space is composed of both camera images and vehicle motion information. Then corresponding dueling neural network structure is introduced, which reduces the variance and improves sampling efficiency. Thirdly, the proposed method is applied to The Open Racing Car Simulator (TORCS) to demonstrate its great performance, where it surpasses human drivers. Finally, the saliency map of the neural network is visualized, which indicates the trained network drives by observing the lane lines. A video for the presented work is available online, <ext-link ext-link-type="uri" href="https://youtu.be/76ciJmIHMD8">https://youtu.be/76ciJmIHMD8</ext-link> or <ext-link ext-link-type="uri" href="https://v.youku.com/v_show/id_XNDM4ODc0MTM4NA==.html">https://v.youku.com/v_show/id_XNDM4ODc0MTM4NA==.html</ext-link> . </p>},
    author = {Baiyu Peng and Qi Sun and Shengbo Eben Li and Dongsuk Kum and Yuming Yin and Junqing Wei and Tianyu Gu},
    doi = {10.1007/s42154-021-00151-3},
    issn = {2096-4250},
    issue = {3},
    journal = {Automotive Innovation},
    keywords = {Deep Q-network,End-to-end autonomous driving,Neural network,Reinforcement learning},
    month = {8},
    pages = {328-337},
    publisher = {Springer Science and Business Media B.V.},
    title = {End-to-End Autonomous Driving Through Dueling Double Deep Q-Network},
    volume = {4},
    url = {https://link.springer.com/10.1007/s42154-021-00151-3},
    year = {2021},
}

@misc{capasso2021endtoend,
    title = {End-to-End Intersection Handling using Multi-Agent Deep Reinforcement Learning},
    author = {A. P. Capasso and P. Maramotti and A. Dellâ€™Eva and A. Broggi},
    year = {2021},
    eprint = {2104.13617},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI}
}

@misc{xu2016endtoend,
    title = {End-to-end Learning of Driving Models from Large-scale Video Datasets},
    author = {H. Xu and Y. Gao and F. Yu and T. Darrell},
    year = {2016},
    eprint = {1612.01079},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}

@misc{toromanoff2019endtoend,
    title = {End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances},
    author = {M. Toromanoff and E. Wirbel and F. Moutarde},
    year = {2019},
    archivePrefix = {arXiv},
    eprint = {1911.10868},
    primaryClass = {cs.RO}
}

@misc{mnih2016asynchronous,
    title = {Asynchronous Methods for Deep Reinforcement Learning},
    author = {V. Mnih and et al.},
    year = {2016},
    eprint = {1602.01783},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{schulman2017proximal,
    title = {Proximal Policy Optimization Algorithms},
    author = {J. Schulman and F. Wolski and P. Dhariwal and A. Radford and O. Klimov},
    year = {2017},
    eprint = {1707.06347},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{schulman2015trust,
    title = {Trust Region Policy Optimization},
    author = {J. Schulman and S. Levine and P. Moritz and M. I. Jordan and P. Abbeel},
    year = {2015},
    eprint = {1502.05477},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{peng2021learning,
    title = {Learning to Simulate Self-Driven Particles System with Coordinated Policy Optimization},
    author = {Z. Peng and Q. Li and K. M. Hui and C. Liu and B. Zhou},
    year = {2021},
    eprint = {2110.13827},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI}
}

@misc{yang2018mean,
    title = {Mean Field Multi-Agent Reinforcement Learning},
    author = {Y. Yang and R. Luo and M. Li and M. Zhou and W. Zhang and J. Wang},
    year = {2018},
    eprint = {1802.05438},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{terry2020pettingzoo,
    title = {PettingZoo: Gym for Multi-Agent Reinforcement Learning},
    author = {J. K. Terry and et al.},
    year = {2020},
    eprint = {2009.14471},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{wu2020cooperative,
    title = {Cooperative Multiagent Deep Deterministic Policy Gradient (CoMADDPG) for Intelligent Connected Transportation with Unsignalized Intersection},
    author = {T. Wu and M. Jiang and L. Zhang},
    journal = {Math Probl Eng},
    volume = {2020},
    year = {2020},
    doi = {10.1155/2020/1820527}
}

@misc{petrazzini2021proximal,
    title = {Proximal Policy Optimization with Continuous Bounded Action Space via the Beta Distribution},
    author = {I. G. B. Petrazzini and E. A. Antonelo},
    year = {2021},
    eprint = {2111.02202},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{lee2021investigating,
    title = {Investigating the state of connected and autonomous vehicles: A literature Review},
    author = {K. M. Lee and S. G. Subramanian and M. Crowley},
    year = {2021},
    eprint = {2111.01100},
    archivePrefix = {arXiv},
    primaryClass = {cs.RO}
}

@article{yang2022inductive,
    title = {An inductive heterogeneous graph attention-based multi-agent deep graph infomax algorithm for adaptive traffic signal control},
    author = {S. Yang and B. Yang},
    journal = {Information Fusion},
    volume = {88},
    pages = {249-262},
    year = {2022},
    month = {12},
    doi = {10.1016/j.inffus.2022.08.001}
}

@article{wang2021adaptive,
    title = {Adaptive Traffic Signal Control for large-scale scenario with Cooperative Group-based Multi-agent reinforcement learning},
    journal = {Transportation Research Part C: Emerging Technologies},
    volume = {125},
    pages = {103046},
    year = {2021},
    issn = {0968-090X},
    doi = {https://doi.org/10.1016/j.trc.2021.103046},
    url = {https://www.sciencedirect.com/science/article/pii/S0968090X21000760},
    author = {Tong Wang and Jiahua Cao and Azhar Hussain},
    keywords = {Multi-agent reinforcement learning, Adaptive traffic signal control, Regional green wave control, CVIS},
    abstract = {Recent research reveals that reinforcement learning can potentially perform optimal decision-making compared to traditional methods like Adaptive Traffic Signal Control (ATSC). With the development of knowledge through trial and error, the Deep Reinforcement Learning (DRL) technique shows its feasibility for the intelligent traffic lights control. However, the general DRL algorithms cannot meet the demands of agents for coordination within large complex road networks. In this article, we introduce a new Cooperative Group-Based Multi-Agent reinforcement learning-ATSC (CGB-MATSC) framework. It is based on Cooperative Vehicle Infrastructure System (CVIS) to realize effective control in the large-scale road network. We propose a CGB-MAQL algorithm that applies k-nearest-neighbor-based state representation, pheromone-based regional green-wave control mode, and spatial discounted reward to stabilize the learning convergence. Extensive experiments and ablation studies of the CGB-MAQL algorithm show its effectiveness and scalability in the synthetic road network, Monaco city and Harbin city scenarios. Results demonstrate that compared with a set of general control methods, our algorithm can better control multiple intersection cases on congestion alleviation and environmental protection.}
}


@article{essa2020selflearning,
    title = {Self-learning adaptive traffic signal control for real-time safety optimization},
    journal = {Accident Analysis \& Prevention},
    volume = {146},
    pages = {105713},
    year = {2020},
    issn = {0001-4575},
    doi = {https://doi.org/10.1016/j.aap.2020.105713},
    url = {https://www.sciencedirect.com/science/article/pii/S0001457520305388},
    author = {Mohamed Essa and Tarek Sayed},
}

@inproceedings{shiwakoti2020investigating,
    title = {Investigating the state of connected and autonomous vehicles: A literature Review},
    author = {N. Shiwakoti and P. Stasinopoulos and F. Fedele},
    booktitle = {Transportation Research Procedia},
    year = {2020},
    pages = {870-882},
    doi = {10.1016/j.trpro.2020.08.101}
}

@article{li2021planning,
    title = {Planning and Decision-making for Connected Autonomous Vehicles at Road Intersections: A Review},
    author = {S. Li and K. Shu and C. Chen and D. Cao},
    journal = {Chinese Journal of Mechanical Engineering},
    volume = {34},
    pages = {133},
    year = {2021},
    doi = {10.1186/s10033-021-00639-3}
}

@misc{gunarathna2022intelligent,
    title = {Intelligent Autonomous Intersection Management},
    author = {U. Gunarathna and S. Karunasekara and R. Borovica-Gajic and E. Tanin},
    year = {2022},
    eprint = {2202.04224},
    archivePrefix = {arXiv},
    primaryClass = {cs.RO}
}

@article{qian2017autonomous,
    author = {Qian, Xiangjun and AltchÃ©, Florent and GrÃ©goire, Jean and de La Fortelle, Arnaud},
    title = {Autonomous Intersection Management systems: criteria, implementation and evaluation},
    journal = {IET Intelligent Transport Systems},
    volume = {11},
    number = {3},
    pages = {182-189},
    keywords = {road vehicles, road traffic control, AIM systems, autonomous vehicles, autonomous intersection management, conflicting evaluation criteria, priority-based design, intersection controller, brake-safe state},
    doi = {https://doi.org/10.1049/iet-its.2016.0043},
    url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-its.2016.0043},
    eprint = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-its.2016.0043},
    abstract = {The foreseeable large-scale deployment of autonomous vehicles in the near future raises the question of autonomous intersection management (AIM). Numerous AIM designs have been proposed, but they lack a common vision of what defines a good system. In this study, the authors discuss a set of conflicting evaluation criteria that need to be balanced in the design of an AIM system, but are often considered individually in the literature. They then introduce their own priority-based design, where an intersection controller assigns priorities to incoming vehicles. On being assigned a priority, vehicles then cross the intersection while maintaining a so-called brake-safe state with respect to higher priority vehicles, rendering the system robust. They have performed extensive simulations to showcase the properties of the proposed system, and notably that it satisfactorily balances their criteria while remaining efficient.},
    year = {2017}
}


@inproceedings{chen2019intersection,
    title = {Intersection Crossing for Autonomous Vehicles based on Deep Reinforcement Learning},
    author = {W.-L. Chen and K.-H. Lee and P.-A. Hsiung},
    booktitle = {2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW)},
    year = {2019},
    pages = {1-2},
    doi = {10.1109/ICCE-TW46550.2019.8991738}
}

@book{sutton2018reinforcement,
    title = {Reinforcement Learning: An Introduction},
    author = {R. S. Sutton and A. G. Barto},
    year = {2018},
    publisher = {MIT Press}
}

@misc{johanson2021emergent,
    title = {Emergent Social Learning via Multi-agent Reinforcement Learning},
    author = {M. B. Johanson and E. Hughes and F. Timbers and J. Z. Leibo},
    year = {2021},
    eprint = {2105.06760},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{tang2020towards,
    title = {Towards Learning Multi-agent Negotiations via Self-Play},
    author = {Y. C. Tang},
    year = {2020},
    eprint = {2001.10208},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{oroojlooyjadid2019review,
    title = {A Review of Cooperative Multi-Agent Deep Reinforcement Learning},
    author = {A. OroojlooyJadid and D. Hajinezhad},
    year = {2019},
    eprint = {1908.03963},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@book{siekmann1998agent,
    title = {Agent and Multi-Agent Systems: Technologies and Applications},
    author = {J. Siekmann and W. Wahlster},
    year = {1998},
    publisher = {Springer}
}

@misc{zhou2020smarts,
    title = {SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving},
    author = {M. Zhou and et al.},
    year = {2020},
    eprint = {2010.09776},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI}
}

@misc{zhang2019multiagent,
    title = {Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms},
    author = {K. Zhang and Z. Yang and T. BaÅŸar},
    year = {2019},
    eprint = {1911.10635},
    archivePrefix = {arXiv},
    primaryClass = {cs.MA}
}

@misc{baker2019emergent,
    title = {Emergent Tool Use From Multi-Agent Autocurricula},
    author = {B. Baker and et al.},
    year = {2019},
    eprint = {1909.07528},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{johanson2022emergent,
    title = {Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning},
    author = {M. B. Johanson and E. Hughes and F. Timbers and J. Z. Leibo},
    year = {2022},
    eprint = {2205.06760},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{baby2022analysis,
    title = {Analysis of Emergent Behavior in Multi Agent Environments using Deep Reinforcement Learning},
    author = {S. A. Baby and L. Li and A. Pokle},
    year = {2022},
    eprint = {},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@article{mnih2015humanlevel,
    title = {Human-level control through deep reinforcement learning},
    author = {V. Mnih and et al.},
    journal = {Nature},
    volume = {518},
    number = {7540},
    pages = {529-533},
    year = {2015},
    doi = {10.1038/nature14236}
}

@misc{paszke2019pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {A. Paszke and et al.},
    year = {2019},
    eprint = {1912.01703},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{li2021metadrive,
    title = {MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning},
    author = {Quanyi Li and Zhenghao Peng and Lan Feng and Qihang Zhang and Zhenghai Xue and Bolei Zhou},
    year = {2022},
    eprint = {2109.12674},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{brockman2016openai,
    title = {OpenAI Gym},
    author = {G. Brockman and et al.},
    year = {2016},
    eprint = {1606.01540},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{bock2019ind,
    title = {The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories at German Intersections},
    author = {J. Bock and R. Krajewski and T. Moers and S. Runde and L. Vater and L. Eckstein},
    year = {2019},
    eprint = {1911.07602},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}

@article{Zhou2022,
    author = {Zhou, Wei and Chen, Dong and Yan, Jun and Li, Zhaojian and Yin, Huilin and Ge, Wanchen},
    title = {Multi-agent reinforcement learning for cooperative lane changing of connected and autonomous vehicles in mixed traffic},
    journal = {Autonomous Intelligent Systems},
    volume = {2},
    number = {1},
    pages = {5},
    year = {2022},
    doi = {10.1007/s43684-022-00023-5},
    url = {https://doi.org/10.1007/s43684-022-00023-5},
    issn = {2730-616X},
    abstract = {Autonomous driving has attracted significant research interests in the past two decades as it offers many potential benefits, including releasing drivers from exhausting driving and mitigating traffic congestion, among others. Despite promising progress, lane-changing remains a great challenge for autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios. Recently, reinforcement learning (RL) has been widely explored for lane-changing decision makings in AVs with encouraging results demonstrated. However, the majority of those studies are focused on a single-vehicle setting, and lane-changing in the context of multiple AVs coexisting with human-driven vehicles (HDVs) have received scarce attention. In this paper, we formulate the lane-changing decision-making of multiple AVs in a mixed-traffic highway environment as a multi-agent reinforcement learning (MARL) problem, where each AV makes lane-changing decisions based on the motions of both neighboring AVs and HDVs. Specifically, a multi-agent advantage actor-critic (MA2C) method is proposed with a novel local reward design and a parameter sharing scheme. In particular, a multi-objective reward function is designed to incorporate fuel efficiency, driving comfort, and the safety of autonomous driving. A comprehensive experimental study is made that our proposed MARL framework consistently outperforms several state-of-the-art benchmarks in terms of efficiency, safety, and driver comfort.}
}

@article{Wang2022,
    title = {Generating merging strategies for connected autonomous vehicles based on spatiotemporal information extraction module and deep reinforcement learning},
    journal = {Physica A: Statistical Mechanics and its Applications},
    volume = {607},
    pages = {128172},
    year = {2022},
    issn = {0378-4371},
    doi = {https://doi.org/10.1016/j.physa.2022.128172},
    url = {https://www.sciencedirect.com/science/article/pii/S0378437122007300},
    author = {Shuo Wang and Hideki Fujii and Shinobu Yoshimura},
    keywords = {Connected and autonomous vehicle, Mixed traffic flow system, Deep reinforcement learning, Longâ€“short term memory neural network, Graph convolution network, Cooperative merging},
    abstract = {A major challenge concerning a mixed traffic flow system, composed of connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs), is how to improve overall efficiency and safety by assigning appropriate control strategies to CAVs. Deep reinforcement learning (DRL) is a promising approach to address this challenge. It enables the joint training of multiple CAVs by fusing CAV sensing information and does not need compliance of HDVs. However, the fusion of CAV sensing information is non-trivial. Traditional DRL models usually fail to take advantage of connectivity among CAVs and time series characteristics of vehicle sensing information, leading to insufficient awareness of the traffic environment. Aimed at tackling these issues, this study proposes a DRL framework named spatiotemporal deep Q network (STDQN), by integrating a double deep Q network (DDQN) and a spatiotemporal information extraction module. A longâ€“short term memory neural network with an attention mechanism (AttenLSTMNN) is leveraged to extract temporal dependencies from vehicle perceptive information. In addition, a graph convolution network (GCN) is employed to model the spatial correlations among vehicles in a local range, as well as the connectivity of multiple CAVs in a global range. Simulation experiments are conducted in an onramp merging scenario, which is one of the most important and commonly seen scenarios in highway or city expressway systems. Experimental results prove that as compared to baseline DRL and rule-based methods, the proposed STDQN can improve the overall traffic efficiency, safety, and driving comfort. The proposed framework is promised to be deployed into real CAVs, to realize cooperative, safe, and efficient autonomous driving.}
}

@INPROCEEDINGS{Isele2017,
    author = {Isele, David and Rahimi, Reza and Cosgun, Akansel and Subramanian, Kaushik and Fujimura, Kikuo},
    booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
    title = {Navigating Occluded Intersections with Autonomous Vehicles Using Deep Reinforcement Learning},
    year = {2018},
    volume = {},
    number = {},
    pages = {2034-2039},
    doi = {10.1109/ICRA.2018.8461233}
}

@misc{Mnih2013,
    title = {Playing Atari with Deep Reinforcement Learning},
    author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
    year = {2013},
    eprint = {1312.5602},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@ARTICLE{9940484,
    author = {Williams, Kyle R. and Schlossman, Rachel and Whitten, Daniel and Ingram, Joe and Musuvathy, Srideep and Pagan, James and Williams, Kyle A. and Green, Sam and Patel, Anirudh and Mazumdar, Anirban and Parish, Julie},
    journal = {IEEE Transactions on Aerospace and Electronic Systems},
    title = {Trajectory Planning With Deep Reinforcement Learning in High-Level Action Spaces},
    year = {2023},
    volume = {59},
    number = {3},
    pages = {2513-2529},
    doi = {10.1109/TAES.2022.3218496} }

@misc{10.3389/fnbot.2022.883562,

    AUTHOR = {Chen, Lienhung and Jiang, Zhongliang and Cheng, Long and Knoll, Alois C. and Zhou, Mingchuan},

    TITLE = {Deep Reinforcement Learning Based Trajectory Planning Under Uncertain Constraints},

    JOURNAL = {Frontiers in Neurorobotics},

    VOLUME = {16},

    YEAR = {2022},

    URL = {https://www.frontiersin.org/articles/10.3389/fnbot.2022.883562},

    DOI = {10.3389/fnbot.2022.883562},

    ISSN = {1662-5218},

    ABSTRACT = {With the advance in algorithms, deep reinforcement learning (DRL) offers solutions to trajectory planning under uncertain environments. Different from traditional trajectory planning which requires lots of effort to tackle complicated high-dimensional problems, the recently proposed DRL enables the robot manipulator to autonomously learn and discover optimal trajectory planning by interacting with the environment. In this article, we present state-of-the-art DRL-based collision-avoidance trajectory planning for uncertain environments such as a safe human coexistent environment. Since the robot manipulator operates in high dimensional continuous state-action spaces, model-free, policy gradient-based soft actor-critic (SAC), and deep deterministic policy gradient (DDPG) framework are adapted to our scenario for comparison. In order to assess our proposal, we simulate a 7-DOF Panda (Franka Emika) robot manipulator in the PyBullet physics engine and then evaluate its trajectory planning with reward, loss, safe rate, and accuracy. Finally, our final report shows the effectiveness of state-of-the-art DRL algorithms for trajectory planning under uncertain environments with zero collision after 5,000 episodes of training.}
}
@article{Fintz2022,
  title = {Using deep learning to predict human decisions and using cognitive models to explain deep learning models},
  author = {Fintz, Matan and Osadchy, Margarita and Hertz, Uri},
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {4736},
  year = {2022},
  doi = {10.1038/s41598-022-08863-0},
  url = {https://doi.org/10.1038/s41598-022-08863-0},
  issn = {2045-2322},
  abstract = {Deep neural networks (DNNs) models have the potential to provide new insights in the study of cognitive processes, such as human decision making, due to their high capacity and data-driven design. While these models may be able to go beyond theory-driven models in predicting human behaviour, their opaque nature limits their ability to explain how an operation is carried out, undermining their usefulness as a scientific tool. Here we suggest the use of a DNN model as an exploratory tool to identify predictable and consistent human behaviour, and using explicit, theory-driven models, to characterise the high-capacity model. To demonstrate our approach, we trained an exploratory DNN model to predict human decisions in a four-armed bandit task. We found that this model was more accurate than two explicit models, a reward-oriented model geared towards choosing the most rewarding option, and a reward-oblivious model that was trained to predict human decisions without information about rewards. Using experimental simulations, we were able to characterise the exploratory model using the explicit models. We found that the exploratory model converged with the reward-oriented modelâ€™s predictions when one option was clearly better than the others, but that it predicted pattern-based explorations akin to the reward-oblivious modelâ€™s predictions. These results suggest that predictable decision patterns that are not solely reward-oriented may contribute to human decisions. Importantly, we demonstrate how theory-driven cognitive models can be used to characterise the operation of DNNs, making DNNs a useful explanatory tool in scientific investigation.}
}

@misc{highway-env,
  author = {Leurent, Edouard},
  title = {An Environment for Autonomous Driving Decision-Making},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/eleurent/highway-env}},
}

@article{HE2021176,
title = {Real-time Energy Optimization of Hybrid Electric Vehicle in Connected Environment Based on Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {10},
pages = {176-181},
year = {2021},
note = {6th IFAC Conference on Engine Powertrain Control, Simulation and Modeling E-COSM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.160},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321015627},
author = {Weiliang He and Ying Huang},
keywords = {Powertrain control, Connected, automated vehicles, Hybrid electric vehicles, Vehicle-to-everything, Deep deterministic policy gradient},
abstract = {In this paper, a real-time control method of hybrid electric vehicle is proposed based on rule-based speed planning and deep deterministic policy gradient (DDPG) energy management algorithm. This method can optimize fuel economy in real time based on all traffic information in a connected environment, and satisfy the constraints of driving safety and driving time. The results show that the proposed deep reinforcement learning algorithm DDPG can achieve lower fuel consumption. In addition, the proposed speed planning algorithm will not violate traffic rules and has good results.}
}


@misc{s20082361,
AUTHOR = {Park, Hyebin and Lim, Yujin},
TITLE = {Reinforcement Learning for Energy Optimization with 5G Communications in Vehicular Social Networks},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {2361},
URL = {https://www.mdpi.com/1424-8220/20/8/2361},
PubMedID = {32326250},
ISSN = {1424-8220},
ABSTRACT = {Increased data traffic resulting from the increase in the deployment of connected vehicles has become relevant in vehicular social networks (VSNs). To provide efficient communication between connected vehicles, researchers have studied device-to-device (D2D) communication. D2D communication not only reduces the energy consumption and loads of the system but also increases the system capacity by reusing cellular resources. However, D2D communication is highly affected by interference and therefore requires interference-management techniques, such as mode selection and power control. To make an optimal mode selection and power control, it is necessary to apply reinforcement learning that considers a variety of factors. In this paper, we propose a reinforcement-learning technique for energy optimization with fifth-generation communication in VSNs. To achieve energy optimization, we use centralized Q-learning in the system and distributed Q-learning in the vehicles. The proposed algorithm learns to maximize the energy efficiency of the system by adjusting the minimum signal-to-interference plus noise ratio to guarantee the outage probability. Simulations were performed to compare the performance of the proposed algorithm with that of the existing mode-selection and power-control algorithms. The proposed algorithm performed the best in terms of system energy efficiency and achievable data rate.},
DOI = {10.3390/s20082361}
}

@article{WALRAVEN2016203,
title = {Traffic flow optimization: A reinforcement learning approach},
journal = {Engineering Applications of Artificial Intelligence},
volume = {52},
pages = {203-212},
year = {2016},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2016.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0952197616000038},
author = {Erwin Walraven and Matthijs T.J. Spaan and Bram Bakker},
keywords = {Traffic flow optimization, Traffic congestion, Variable speed limits, Reinforcement learning, Neural networks},
abstract = {Traffic congestion causes important problems such as delays, increased fuel consumption and additional pollution. In this paper we propose a new method to optimize traffic flow, based on reinforcement learning. We show that a traffic flow optimization problem can be formulated as a Markov Decision Process. We use Q-learning to learn policies dictating the maximum driving speed that is allowed on a highway, such that traffic congestion is reduced. An important difference between our work and existing approaches is that we take traffic predictions into account. A series of simulation experiments shows that the resulting policies significantly reduce traffic congestion under high traffic demand, and that inclusion of traffic predictions improves the quality of the resulting policies. Additionally, the policies are sufficiently robust to deal with inaccurate speed and density measurements.}
}


@misc{s22176655,
AUTHOR = {Shahi, Saugat and Lee, Heoncheol},
TITLE = {Autonomous Rear Parking via Rapidly Exploring Random-Tree-Based Reinforcement Learning},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {17},
ARTICLE-NUMBER = {6655},
URL = {https://www.mdpi.com/1424-8220/22/17/6655},
PubMedID = {36081115},
ISSN = {1424-8220},
ABSTRACT = {This study addresses the problem of autonomous rear parking (ARP) for car-like nonholonomic vehicles. ARP includes path planning to generate an efficient collision-free path from the start point to the target parking slot and path following to produce control inputs to stably follow the generated path. This paper proposes an efficient ARP method that consists of the following five components: (1) OpenAI Gym environment for training the reinforcement learning agent, (2) path planning based on rapidly exploring random trees, (3) path following based on model predictive control, (4) reinforcement learning based on the Markov decision process, and (5) travel length estimation between the start and the goal points. The evaluation results in OpenAI Gym show that the proposed ARP method can successfully be used by minimizing the difference between the reference points and trajectories produced by the proposed method.},
DOI = {10.3390/s22176655}
}






